import logging
from typing import List, cast, Literal
from openai import OpenAI
from openai.types.chat import (
    ChatCompletionMessageParam,
    ChatCompletionSystemMessageParam as SysMessage,
    ChatCompletionAssistantMessageParam as AssistantMessage,
    ChatCompletionUserMessageParam as UserMessage,
)
from persons.person import Person
from session_rooms.session_room import ChatEntry
from session_rooms.session_room import System
import time


log = logging.getLogger(__name__)


class PersonVLLM(Person):
    PERSON_TYPE = "person_vllm"

    def __init__(
        self,
        background_story: str,
        you_background_story: str,
        name: str,
        prompt_version: str = "v0",
        *args,
        **kwargs,
    ):
        super().__init__(background_story,you_background_story, name)
        self.api_base: str = kwargs.get("vllm_api_base", "http://localhost:8001/v1")
        
        # self.model: str = kwargs.get(
        #     "model", "mistralai/Mistral-Small-3.1-24B-Instruct-2503"
        # )
        self.model: str = kwargs.get("model", "openai/gpt-oss-120b")
        self.client = OpenAI(
            api_key="EMPTY",  # vLLM usually ignores this, but required by the client
            base_url=self.api_base,
        )

        self.prompt_version = prompt_version

    def generate_answer(
        self,
        experiment_scenario: str,
        chat_list: list[ChatEntry],
        prompt_version: str | None = None,
        is_questionnaire: bool = False,
    ):
        if prompt_version is None:
            prompt_version = self.prompt_version
        messages: List[ChatCompletionMessageParam] = self.create_prompt(
            experiment_scenario, chat_list, prompt_version, is_questionnaire
        )
        
        answer = self.evaluate(messages)

        return ChatEntry(entity=self, prompt=messages, answer=answer)

    def evaluate(self, messages: List[ChatCompletionMessageParam], max_new_tokens=100):

        cnt = 0
        max_cnt = 3
        response = None

        while response is None:

            
            try:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    n=1,
                    temperature=0.1,
                )
            except Exception as e:
                # wait for 2 seconds and try again
                log.error(f"Error while calling vLLM API: {e}. Retrying...")
                log.error(f"Messages: {messages}")
                time.sleep(15)
                cnt += 1

                if cnt >= max_cnt:
                    log.error(f"Failed to get response from vLLM API after {max_cnt} attempts.")
                    break



        output = (response.choices[0].message.content or "") if response.choices else ""
        # remove the "Me: " prefix from the answer
        return (
            output.strip().removeprefix("Me: ").removeprefix(f"{self.name}: ").strip()
        )

    # TODO: Choose the best prompt and prompt structure (should it all be in system?)
    def create_prompt(
        self, experiment_scenario: str, chat_list: List[ChatEntry], prompt_version: str, is_questionnaire: bool = False
    ) -> List[ChatCompletionMessageParam]:
        """
        Creates a prompt with the past conversation in the format expected by OpenAI Chat API.
        The returned conversation is a list of entries, which follows the format described at
        https://help.openai.com/en/articles/7042661-chatgpt-api-transition-guide.

        In particular, the "role" property has 3 values, which we use as follows:
            - "system": Only used in the first / last entries to set up the person instance identity.
            - "assistant": Used for messages generated by the person instance.
            - "user": Used for messages generated by other persons. Each entry can consist of
              messages from multiple persons, by concatenating the format "{name}: {content}\n".
        """

        assert prompt_version in [
            "v0",
            "v1",
            "v2",
        ], f"Unknown prompt version {prompt_version}. Please use v0, v1 or v2."
        prompt_version_literal: Literal["v0", "v1", "v2"] = cast(
            Literal["v0", "v1", "v2"], prompt_version
        )
        conversation: List[ChatCompletionMessageParam] = super().prompt_setups(
            experiment_scenario=experiment_scenario,
            prompt_version=prompt_version_literal,
            is_questionnaire=is_questionnaire,
        )

        for chat_entry in chat_list:
            if isinstance(chat_entry.entity, System):  # System message
                conversation.append(UserMessage(role="user", content=chat_entry.answer))
            elif chat_entry.entity.name == self.name:  # This person's message
                # person's message
                conversation.append(
                    AssistantMessage(
                        role="assistant",
                        content=f"{chat_entry.answer}\n",
                    )
                )
            else:  # Other person's message
                # Concatenate the name and content of the other person's message
                conversation.append(
                    UserMessage(
                        role="user",
                        content=f"{chat_entry.answer}\n",
                    )
                )

        return conversation
